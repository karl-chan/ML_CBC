
mc\lr	  0.1       0.2       0.3       0.4       0.5       0.6       0.7       0.8       0.9
=============================================================================================
0.0    0.4399    0.5275    0.5963    0.6290    0.5847    0.6107    0.5947    0.5680    0.5473
0.1    0.4686    0.5258    0.5694    0.6296    0.6173    0.6187    0.6328    0.5834    0.6273
0.2    0.4325    0.5200    0.6144    0.6477    0.6331    0.6514    0.6378    0.6437    0.6495
0.3    0.4455    0.5409    0.5638    0.6334    0.6060    0.6327    0.6673    0.6924    0.6672
0.4    0.4164    0.5329    0.5884    0.5974    0.6020    0.6371    0.6812    0.6542    0.6748
0.5    0.4721    0.5369    0.5525    0.5953    0.6170    0.6278    0.6895    0.6824    0.6496
0.6    0.4473    0.5053    0.5578    0.5879    0.5553    0.6593    0.6342    0.6868    0.6927
0.7    0.4092    0.5049    0.5549    0.5701    0.6103    0.6534    0.6213    0.6541    0.6571
0.8    0.3405    0.4321    0.5616    0.5855    0.6168    0.6138    0.6189    0.6702    0.6404
0.9    0.1483    0.1865    0.1578    0.1863    0.1986    0.2964    0.2179    0.2294    0.3538

Optimal at (lr, mc) = (0.9, 0.6). F1 measure = 0.6927.

For fixed optimal parameters: optimal hidden neurons (single layer) is 50, F1 measure is 0.8207.


________________________________________________________________________________________________

Second attempt:

lr = 0.01:0.01:0.11
mc = 0.0:0.1:0.9

Optimal hidden neurons (single layer) is 30

OPTIMAL:
 -- F1: 0.8232
 -- lr: 0.10
 -- mc: 0.4

10 x 11 matrix: row -> mc; col -> lr
m = reshape(results, [10 11])

lr = 0.01:
    0.6389
    0.6494
    0.6310
    0.6742
    0.6597
    0.6278
    0.6655
    0.6455
    0.6292
    0.6358
    
lr = 0.02:
    0.7442
    0.7260
    0.7289
    0.7446
    0.7360
    0.7388
    0.7321
    0.7200
    0.6966
    0.6167
    
    0.7599
    0.7574
    0.7467
    0.7597
    0.7527
    0.7512
    0.7676
    0.7567
    0.7547
    0.6758
    
    0.7651
    0.7630
    0.7729
    0.7572
    0.7772
    0.7768
    0.7809
    0.7857
    0.7817
    0.3296
    
    0.7773
    0.7841
    0.7887
    0.7811
    0.7923
    0.7670
    0.7816
    0.7773
    0.7932
    0.2076
    
    0.7978
    0.7966
    0.8143
    0.8009
    0.7914
    0.7828
    0.8158
    0.7990
    0.8045
    0.3195
    
    0.8003
    0.7987
    0.8075
    0.7996
    0.8090
    0.7968
    0.8112
    0.8070
    0.8110
    0.1548
    
    0.8083
    0.8054
    0.8045
    0.8046
    0.8148
    0.8090
    0.8032
    0.7934
    0.6724
    0.2943
    
    0.8162
    0.8064
    0.8021
    0.8068
    0.8134
    0.8114
    0.8176
    0.8104
    0.7391
    0.1934
    
    0.8061
    0.8056
    0.8124
    0.8212
    0.8232
    0.8085
    0.8140
    0.8095
    0.6148
    0.2532
    
lr = 0.11
    0.8090
    0.8034
    0.8126
    0.8049
    0.7992
    0.8121
    0.8175
    0.8189
    0.6966
    0.1381

